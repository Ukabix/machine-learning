
setup:
python libraries:
numpy,
pandas,
matplotlib,
sklearn, / simpleimputer

environment:
Spyder > Pycharm? Variable explorer, ctrl+i are very strong tools!


Independent variables / Dependent variables
Practical hint - replace missing number data with mean of other values
dataset.iloc[line:line,col:col]

linear regression - simple y = a*x + c, conditions:
a) linearity (liniowość - oczywiste, wykładnik a^1)
b) homoscedasticity / heteroscadisticity (stałość przewidywania przez model zmiennej zależnej dla próżnych wartości zmiennej niezależnej. http://www.naukowiec.org/wiedza/statystyka/homoscedastycznosc_533.html)
c) multivariate normality
d) independence of errors
e) lack of multicollinearity (wielowspółliniowość?)

multiple linear regression
categorical data - data that can sorted into categories where (jak klasyczna klasyfikacja!)
dummy variables for categorical vars:
create a matrix where col = categories, populate with 1 and 0 - work like switches

Support Vector Regression - SVR
- different regression goal - minimize the error between prediction and data
- classifies all predictor lines into 2 classes - 1 that pass the threshold (Epsilon), 2 that don't (discard this data)


!dummy variable trap - always omit one dummy variable
example
if
y = b0 + b1*x1 + b2*x2 + b3*x3 + b4*D1 + b5*D2
since
D1, D2 are independent dummy vars
D2 = 1 - D1
then
the model has different results from dummy vars
so
the model can't distinguish the effects of D1 from the effects of D2!
alas
homoscedasticity condition for linear regression is not met
QED

Building a model for multiple linear regression:
1. All in
2. Backward elimination
3. Forward selection
4. Bidirectional elimination
5. Score comparison
(2-4) stepwise regression

Interesting topics to dive into:
information entropy,
ensemble learning - using multiple algorithms (or single one) like multiple DT to form and advanced tool (like RFR)


CLASSIFICATION:

Logistic regression:
Probability can be expressed in range <0,1> when all indep variables == 0 or 1 (yes/no)
So we can model from linear regression, where for all x, any y: lim y -> 0 or lim y -> 1 (cut away parts <0 and >1)

Since:
y = b0 + b1*x
and the sigmoid function for prob is:
p = 1 / 1 + e^-y
we get:
ln( p / (1 - p) ) = b0 + b1*x
Logistic regression equation ^
Easy :)

In practice we use probability as a score.

The main aim of this method is to classify the users to the right categories (pred regions)!

Classifiers:
I. Line regression (linear classifier / linear boundry)

II. K - NN (Nearest neighbour):
1) choose k no of neighbours (def 5)
2) take k nearest neighbours of the new data point (Euclidean dist?)
3) among K NN count the nr of data points in each category
4) assign the new data point to the category where most NN are in

III. SVM - Support Vector Machine -
Strong sides - works well in extremes:
Normal models - takes an ordinary stock as reference -> prediction how to classify next object
SVM - takes data stock from the greyish area -> prediction
1) Linear, creating positive and negative hyperlanes that are d > |max margin| from the margin line
2) Nonlinear - mapping data to a higher diemension via mapping function - CPU INTENSIVE!

!Kernel trick:
Gaussian RBF:
K(x, l^i) = e ^- (|x-l^i|^2) / 2sigma^2
To check - Need to determine limes of K for given x -> 0 V x -> 1, relative to the landmark
Also:
sigma -> unl => wider base of the model
sigma -> 0 => narrow base of the model
Data outside the base = 0

Other kernels: sigmoid, polynomial

IV. Bayes Theorem
P(A|B) = (P(B|A) * P(A)) / P(B)
Spanner case:
m1: 30 / hr -> P(m1) = 30/50 = 0.6
m2: 20 / hr -> P(m2) = 20/50 = 0.4
of all parts, 1% of all are defective -> P (Defect) = 1%
of def parts, 50% come from m1, 50% from m2
-> P(m1 | defect) = 50%
-> P(m2 | defect) = 50%
what is the prob that a part produced by m2 is defective?
-> P(defect | m2) = ?

We need:
P(m2) = 0.4
P(def) = 1%
P(m2|def) = 50%
P(def|m2) = ?

BT Solution:
P(def|m2) = (P(m2|def) * P(def))/P(m2)

P = (0.5*0.01)/0.4 = 0.005/0.4 = 0.0125 = 1,25%

Naive Beyes - based upon an assumption of no feature correlation

V. Decision Tree Classification
Splits/leaves are made in such a way to maximize the amount of similar data and decrease information entropy.
- Generally an old method, but reborn thanks to upgrades that make them powerful
- Random Forest, Gradient Boosting etc.
- NOT BASED ON EUCLIDEAN DISTANCE!, most are based on entropy

VI. Random Forest Classification Intuition
Ensemble Learning - combines many ML methods into one
1. Pick random K data points from Train set
2. Build a DT associated to K points
3. Choose num of Ntree trees you want to build and rep 1&2
4. For new data point, make each one of your Ntrees predict the category to which the data points belong
and assign the new data point to the category that wins the majority vote.
Kinnect is based on it.
Overfitting is an issue (when you fit too much training data into classifier that it gets lost handling data it is not used to)

Model evaluation:
error type I - false positive, error type II - false neg (more serious)
confusion matrix
cumulative accuracy profile != Receiver operating character
CAP curve analysis:
AR = Ar/ap
or take the 50% pop and check the chart of %:
X<60% - rubbish
60<X<70 - poor
70<X<80 - good
80<X<90 - very good
90<X - too good, something is probably wrong (overfitting)

CLUSTERING:
I. K-means

Plan of attack:
1. Choose num K of clusters
2. Select random K points - the centroids
3. Assign each data point closest to the closest centroid (centre of weight) -> clusters formed
4. Compute and place the new centroid for each cluster
5. Reassign each data point to the new closest centroid.
If any reassignment took place, goto 4, other fin.

Random Initialization Trap:
Bad random init - the selection of the centroids can predict the outcome in some cases (false reclustering)
The solution is to use K-Means++ instead.

Choosing the number of clusters:
Within Cluster Sum of Squares WCSS:
General rule is that lower WCSS is better
WCSS = 0 == cluster num = data points
Optimal WCSS according to elbow method - compare wcss for cluster num on a chart and look for the slope

II. Hierarchical
Aggregational(bot -> top) and divisive (top -> bottom)

Plan of attack:
1. Make each data point a single point cluster -> get N clusters
2. Take 2 closest (proximity of clusters) data points and make them 1 cluster -> N-1 clusters
3. Take 2 closest clusters and make them 1 cluster -> get N-2 clusters
4. Repeat 3 until there is only 1 cluster

Measuring distance between 2 clusters:
1. Closest points
2. Furthest points
3. Average distance
4. Distance between centroids
Note- this impacts the result.

Dendograms and dissimilarity line
- represented via f ( Euclidean dist | data points )
- take a threshold that passes through the largest distance of a vertical line that is not cut via any horizontal on the graph


ASSOCIATION RULE LEARNING

I. Apriori intuition
- based on available data to draw connections

Plan of Attack:
Support -> Confidence -> Lift

Support:
support(M) = #user watchlists cointaining M / # user watchlists
10 of 100 seen Exmachina = 10/100=10%
Confidence:
confidence(M1->M2) = # user watchlists containing M1 & M2 / # user watchlists containing M1
40 of 100 seen Interstellar
7 of them saw Exmachina, so 7/40 = 17.5%
Lift - improvement of prediction
lift(M1->M2) = confidence (M1->M2) / support (M)
0.175 / 0.1 = 1.75

^ similar to Naive Beyes

Algorithm:
1) Set min Supp and Conf
2) Take all subsets in transactions having higher support than min support
3) Take all the rules of these subsets having higher confidence than minimum confidence
4) Sort the rules by decreasing lift

REINFORCEMENT RULE LEARNING:
- based on reward/score system

Plan of attack:
1. At each round n, consider two num for each ad i:
- Ni(n) - the num of times the ad i was selected up to round n,
- Ri(n) - the sum of rewards of the ad i up to round n
2. From these two num we compute:
- average reward of ad i up to round n ri(n) == Ri(n)/Ni(n)
- the confidence unterval |ri(n) - deltai (n), ri(n) + delta i (n)
with delta i (n) = sqrt [ 3 log (n) / 2 Ni(n) ]
3. we select the ad i that has the max UCB = ri(n) + deltai (n)
