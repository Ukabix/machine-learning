
setup:
python libraries:
numpy,
pandas,
matplotlib,
sklearn, / simpleimputer

environment:
Spyder > Pycharm? Variable explorer, ctrl+i are very strong tools!


Independent variables / Dependent variables
Practical hint - replace missing number data with mean of other values
dataset.iloc[line:line,col:col]

linear regression - simple y = a*x + c, conditions:
a) linearity (liniowość - oczywiste, wykładnik a^1)
b) homoscedasticity / heteroscadisticity (stałość przewidywania przez model zmiennej zależnej dla próżnych wartości zmiennej niezależnej. http://www.naukowiec.org/wiedza/statystyka/homoscedastycznosc_533.html)
c) multivariate normality
d) independence of errors
e) lack of multicollinearity (wielowspółliniowość?)

multiple linear regression
categorical data - data that can sorted into categories where (jak klasyczna klasyfikacja!)
dummy variables for categorical vars:
create a matrix where col = categories, populate with 1 and 0 - work like switches

Support Vector Regression - SVR
- different regression goal - minimize the error between prediction and data
- classifies all predictor lines into 2 classes - 1 that pass the threshold (Epsilon), 2 that don't (discard this data)


!dummy variable trap - always omit one dummy variable
example
if
y = b0 + b1*x1 + b2*x2 + b3*x3 + b4*D1 + b5*D2
since
D1, D2 are independent dummy vars
D2 = 1 - D1
then
the model has different results from dummy vars
so
the model can't distinguish the effects of D1 from the effects of D2!
alas
homoscedasticity condition for linear regression is not met
QED

Building a model for multiple linear regression:
1. All in
2. Backward elimination
3. Forward selection
4. Bidirectional elimination
5. Score comparison
(2-4) stepwise regression

Interesting topics to dive into:
information entropy,
ensemble learning - using multiple algorithms (or single one) like multiple DT to form and advanced tool (like RFR)


CLASSIFICATION:

Logistic regression:
Probability can be expressed in range <0,1> when all indep variables == 0 or 1 (yes/no)
So we can model from linear regression, where for all x, any y: lim y -> 0 or lim y -> 1 (cut away parts <0 and >1)

Since:
y = b0 + b1*x
and the sigmoid function for prob is:
p = 1 / 1 + e^-y
we get:
ln( p / (1 - p) ) = b0 + b1*x
Logistic regression equation ^
Easy :)

In practice we use probability as a score.

The main aim of this method is to classify the users to the right categories (pred regions)!

Classifiers:
I. Line regression (linear classifier / linear boundry)

II. K - NN (Nearest neighbour):
1) choose k no of neighbours (def 5)
2) take k nearest neighbours of the new data point (Euclidean dist?)
3) among K NN count the nr of data points in each category
4) assign the new data point to the category where most NN are in

III. SVM - Support Vector Machine -
Strong sides - works well in extremes:
Normal models - takes an ordinary stock as reference -> prediction how to classify next object
SVM - takes data stock from the greyish area -> prediction
1) Linear, creating positive and negative hyperlanes that are d > |max margin| from the margin line
2) Nonlinear - mapping data to a higher diemension via mapping function - CPU INTENSIVE!

!Kernel trick:
Gaussian RBF:
K(x, l^i) = e ^- (|x-l^i|^2) / 2sigma^2
To check - Need to determine limes of K for given x -> 0 V x -> 1, relative to the landmark
Also:
sigma -> unl => wider base of the model
sigma -> 0 => narrow base of the model
Data outside the base = 0

Other kernels: sigmoid, polynomial

IV. Bayes Theorem
P(A|B) = (P(B|A) * P(A)) / P(B)
Spanner case:
m1: 30 / hr -> P(m1) = 30/50 = 0.6
m2: 20 / hr -> P(m2) = 20/50 = 0.4
of all parts, 1% of all are defective -> P (Defect) = 1%
of def parts, 50% come from m1, 50% from m2
-> P(m1 | defect) = 50%
-> P(m2 | defect) = 50%
what is the prob that a part produced by m2 is defective?
-> P(defect | m2) = ?

We need:
P(m2) = 0.4
P(def) = 1%
P(m2|def) = 50%
P(def|m2) = ?

BT Solution:
P(def|m2) = (P(m2|def) * P(def))/P(m2)

P = (0.5*0.01)/0.4 = 0.005/0.4 = 0.0125 = 1,25%

Naive Beyes - based upon an assumption of no feature correlation

V. Decision Tree Classification
Splits/leaves are made in such a way to maximize the amount of similar data and decrease information entropy.
- Generally an old method, but reborn thanks to upgrades that make them powerful
- Random Forest, Gradient Boosting etc.
- NOT BASED ON EUCLIDEAN DISTANCE!, most are based on entropy

VI. Random Forest Classification Intuition
Ensemble Learning - combines many ML methods into one
1. Pick random K data points from Train set
2. Build a DT associated to K points
3. Choose num of Ntree trees you want to build and rep 1&2
4. For new data point, make each one of your Ntrees predict the category to which the data points belong
and assign the new data point to the category that wins the majority vote.
Kinnect is based on it.
Overfitting is an issue (when you fit too much training data into classifier that it gets lost handling data it is not used to)
