
setup:
python libraries:
numpy,
pandas,
matplotlib,
sklearn, / simpleimputer

environment:
Spyder > Pycharm? Variable explorer, ctrl+i are very strong tools!


Independent variables / Dependent variables
Practical hint - replace missing number data with mean of other values
dataset.iloc[line:line,col:col]

linear regression - simple y = a*x + c, conditions:
a) linearity (liniowość - oczywiste, wykładnik a^1)
b) homoscedasticity / heteroscadisticity (stałość przewidywania przez model zmiennej zależnej dla próżnych wartości zmiennej niezależnej. http://www.naukowiec.org/wiedza/statystyka/homoscedastycznosc_533.html)
c) multivariate normality
d) independence of errors
e) lack of multicollinearity (wielowspółliniowość?)

multiple linear regression
categorical data - data that can sorted into categories where (jak klasyczna klasyfikacja!)
dummy variables for categorical vars:
create a matrix where col = categories, populate with 1 and 0 - work like switches

Support Vector Regression - SVR
- different regression goal - minimize the error between prediction and data
- classifies all predictor lines into 2 classes - 1 that pass the threshold (Epsilon), 2 that don't (discard this data)

Random Forest Intuition
-

!dummy variable trap - always omit one dummy variable
example
if
y = b0 + b1*x1 + b2*x2 + b3*x3 + b4*D1 + b5*D2
since
D1, D2 are independent dummy vars
D2 = 1 - D1
then
the model has different results from dummy vars
so
the model can't distinguish the effects of D1 from the effects of D2!
alas
homoscedasticity condition for linear regression is not met
QED

Building a model for multiple linear regression:
1. All in
2. Backward elimination
3. Forward selection
4. Bidirectional elimination
5. Score comparison
(2-4) stepwise regression

Interesting topics to dive into:
information entropy,
ensemble learning - using multiple algorithms (or single one) like multiple DT to form and advanced tool (like RFR)

